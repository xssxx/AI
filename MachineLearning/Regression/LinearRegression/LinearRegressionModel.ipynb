{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "FhctJrU18d24"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**"
      ],
      "metadata": {
        "id": "4T4WeqAPJaeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, epochs, learning_rate):\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = []\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.w) + self.b\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.w = np.zeros(x.shape[1])\n",
        "        self.b = 0\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            y_pred = self.predict(x)\n",
        "            error = y_pred - y\n",
        "\n",
        "            \"\"\"\n",
        "            Partial derivative\n",
        "            dError/dW                                       ไส้\n",
        "                = 2 * (error) * ดิฟไส้   |  diff(x.T * w + b - y) = x.T\n",
        "                = 2 * (error) * x.T\n",
        "            \"\"\"\n",
        "            grad_w = 2 * np.dot(x.T, error) / len(x)\n",
        "            grad_b = 2 * np.sum(error) / len(x)\n",
        "\n",
        "            self.w = self.w - grad_w * self.learning_rate\n",
        "            self.b = self.b - grad_b * self.learning_rate\n",
        "\n",
        "            mse = np.mean((error) ** 2)\n",
        "            self.loss.append(mse)\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(f'epoch {i+1}/{self.epochs} loss = {mse}')\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.w) + self.b\n",
        "\n",
        "    def get_loss(self):\n",
        "        return self.loss\n",
        "\n",
        "    def get_weight(self):\n",
        "        return self.w\n",
        "\n",
        "    def get_bias(self):\n",
        "        return self.b"
      ],
      "metadata": {
        "id": "omZpqSOhJLay"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(12)\n",
        "\n",
        "x = np.random.rand(3, 3)\n",
        "y = np.array([i*3 for i in x])\n",
        "\n",
        "model = LinearRegression(learning_rate=0.01, epochs=10000)\n",
        "model.fit(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIgHKohrLY1W",
        "outputId": "521a4600-3574-4f43-be10-dc31659df95d"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/10000 loss = 3.4981170394239802\n",
            "epoch 1001/10000 loss = 0.029476569873141338\n",
            "epoch 2001/10000 loss = 0.017461450686365215\n",
            "epoch 3001/10000 loss = 0.010524800605339094\n",
            "epoch 4001/10000 loss = 0.006365133633798873\n",
            "epoch 5001/10000 loss = 0.0038612405129314947\n",
            "epoch 6001/10000 loss = 0.002348825171802618\n",
            "epoch 7001/10000 loss = 0.001432395341520198\n",
            "epoch 8001/10000 loss = 0.0008754948796695983\n",
            "epoch 9001/10000 loss = 0.0005361916988130507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(x)\n",
        "x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBiernZ1ROWK",
        "outputId": "921d0fb3-58a7-4a5f-d4ff-5db1a064724c"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.15416284, 0.7400497 , 0.26331502],\n",
              "        [0.53373939, 0.01457496, 0.91874701],\n",
              "        [0.90071485, 0.03342143, 0.95694934]]),\n",
              " array([[0.46248853, 2.22014909, 0.78994505],\n",
              "        [1.60121818, 0.04372489, 2.75624102],\n",
              "        [2.70214456, 0.10026428, 2.87084801]]))"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH0OZJ2Wb9lq",
        "outputId": "485af8fd-7b89-46d2-f966-61f7cc94bae5"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46146487, 2.21918194, 0.78903014],\n",
              "       [1.63724609, 0.04991958, 2.73484886],\n",
              "       [2.6730195 , 0.09510539, 2.8878507 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kduJ4G9PdSRl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}